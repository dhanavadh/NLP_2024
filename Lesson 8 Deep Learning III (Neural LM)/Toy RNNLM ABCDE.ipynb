{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kfV9EHiznq2z"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import random\n","#import tiktoken  # Use TikToken for fast tokenization"]},{"cell_type":"code","source":["# prompt: generate a 1 million character strings of ABCDE. A C can occur anywhere. But B will occur only after AA and D will only come after CCCC. E can occur only after AC.\n","\n","import random\n","\n","def generate_string(length):\n","  \"\"\"\n","  Generates a string of specified length with the given constraints:\n","  - C can occur anywhere.\n","  - B occurs only after AA.\n","  - D occurs only after CCCC.\n","  - E occurs only after AC.\n","  \"\"\"\n","  result = \"\"\n","  for _ in range(length):\n","    valid_chars = ['A', 'C']\n","    if len(result) >= 2 and result[-2:] == \"AA\":\n","        valid_chars.append(\"B\")\n","    if len(result) >= 4 and result[-4:] == \"CCCC\":\n","        valid_chars.append(\"D\")\n","    if len(result) >= 2 and result[-2:] == 'AC':\n","        valid_chars.append(\"E\")\n","\n","\n","    result += random.choice(valid_chars)\n","\n","  return result\n","\n","# Generate a 1 million character string\n","text_data = generate_string(1000000)\n","print(len(text_data))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFt3IOcVo4wv","executionInfo":{"status":"ok","timestamp":1738636883043,"user_tz":-420,"elapsed":542,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"b88cadc6-9be3-43bb-ae64-3db010db7c17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1000000\n"]}]},{"cell_type":"code","source":["import collections\n","\n","distribution = collections.Counter(text_data)\n","\n","# Print the distribution\n","distribution\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F8FsBfctpLh5","executionInfo":{"status":"ok","timestamp":1738636883043,"user_tz":-420,"elapsed":7,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"642fa21b-c20b-4274-c8cc-6291348558ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'A': 432500, 'B': 61860, 'C': 432438, 'E': 61240, 'D': 11962})"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["class RNNLM(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, dropout):\n","        super(RNNLM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.rnn = nn.GRU(embed_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x, hidden):\n","        x = self.embedding(x)\n","        out, hidden = self.rnn(x, hidden)\n","        out = self.fc(out)\n","        return out, hidden\n"],"metadata":{"id":"aVMTDVcJpWun"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","tokenizer = {x:i for i,x in enumerate(set(text_data))}\n","class TextDataset(Dataset):\n","    def __init__(self, tokens, seq_len):\n","        self.tokens = [tokenizer[x] for x in tokens]\n","        self.seq_len = seq_len\n","\n","    def __len__(self):\n","        return len(self.tokens) - self.seq_len\n","\n","    def __getitem__(self, idx):\n","        x = torch.tensor(self.tokens[idx:idx+self.seq_len], dtype=torch.long)\n","        y = torch.tensor(self.tokens[idx+1:idx+self.seq_len+1], dtype=torch.long)\n","        return x, y"],"metadata":{"id":"1nqTorL6pbKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","BATCH_SIZE = 64\n","SEQ_LEN = 128\n","EMBED_DIM = 16\n","HIDDEN_SIZE = 32\n","NUM_LAYERS = 1\n","DROPOUT = 0.3\n","LR = 3e-4\n","EPOCHS = 3\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","VOCAB_SIZE = len(set(text_data))"],"metadata":{"id":"om9Ul7bgpkaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = TextDataset(text_data, SEQ_LEN)\n","dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BqJxfwup30W","executionInfo":{"status":"ok","timestamp":1738636936749,"user_tz":-420,"elapsed":4,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"c6359dfd-1141-4cc6-f65f-bd1c28de7166"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["model = RNNLM(VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(DEVICE)\n","optimizer = optim.AdamW(model.parameters(), lr=LR)\n","criterion = nn.CrossEntropyLoss()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OxZsTHwVqAFh","executionInfo":{"status":"ok","timestamp":1738636940075,"user_tz":-420,"elapsed":598,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"2f203528-cebe-46c5-e0e9-7cf607b9619d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Training loop\n","def train():\n","    model.train()\n","    for epoch in range(EPOCHS):\n","        hidden = None\n","        for batch_idx, (x, y) in enumerate(dataloader):\n","            x, y = x.to(DEVICE), y.to(DEVICE)\n","            optimizer.zero_grad()\n","            prediction, hidden = model(x, hidden)\n","            hidden = hidden.detach()\n","            loss = criterion(prediction.view(-1, VOCAB_SIZE), y.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch_idx % 100 == 0:\n","                print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}')\n","\n","        # Save model checkpoint\n","        torch.save(model.state_dict(), f'rnnlm_epoch{epoch+1}.pth')\n"],"metadata":{"id":"fcRd1VoTqXHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMqv8s5uryYY","executionInfo":{"status":"ok","timestamp":1738637185044,"user_tz":-420,"elapsed":238600,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"1691ada3-2371-4ece-ea2e-2fddebb3aba4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [0/15623], Loss: 1.6406\n","Epoch [1/3], Step [100/15623], Loss: 1.1744\n","Epoch [1/3], Step [200/15623], Loss: 1.0703\n","Epoch [1/3], Step [300/15623], Loss: 1.0003\n","Epoch [1/3], Step [400/15623], Loss: 0.9514\n","Epoch [1/3], Step [500/15623], Loss: 0.9310\n","Epoch [1/3], Step [600/15623], Loss: 0.9149\n","Epoch [1/3], Step [700/15623], Loss: 0.8986\n","Epoch [1/3], Step [800/15623], Loss: 0.8908\n","Epoch [1/3], Step [900/15623], Loss: 0.8829\n","Epoch [1/3], Step [1000/15623], Loss: 0.8792\n","Epoch [1/3], Step [1100/15623], Loss: 0.8750\n","Epoch [1/3], Step [1200/15623], Loss: 0.8756\n","Epoch [1/3], Step [1300/15623], Loss: 0.8690\n","Epoch [1/3], Step [1400/15623], Loss: 0.8669\n","Epoch [1/3], Step [1500/15623], Loss: 0.8716\n","Epoch [1/3], Step [1600/15623], Loss: 0.8700\n","Epoch [1/3], Step [1700/15623], Loss: 0.8714\n","Epoch [1/3], Step [1800/15623], Loss: 0.8699\n","Epoch [1/3], Step [1900/15623], Loss: 0.8695\n","Epoch [1/3], Step [2000/15623], Loss: 0.8651\n","Epoch [1/3], Step [2100/15623], Loss: 0.8636\n","Epoch [1/3], Step [2200/15623], Loss: 0.8645\n","Epoch [1/3], Step [2300/15623], Loss: 0.8651\n","Epoch [1/3], Step [2400/15623], Loss: 0.8639\n","Epoch [1/3], Step [2500/15623], Loss: 0.8633\n","Epoch [1/3], Step [2600/15623], Loss: 0.8671\n","Epoch [1/3], Step [2700/15623], Loss: 0.8650\n","Epoch [1/3], Step [2800/15623], Loss: 0.8685\n","Epoch [1/3], Step [2900/15623], Loss: 0.8587\n","Epoch [1/3], Step [3000/15623], Loss: 0.8623\n","Epoch [1/3], Step [3100/15623], Loss: 0.8675\n","Epoch [1/3], Step [3200/15623], Loss: 0.8627\n","Epoch [1/3], Step [3300/15623], Loss: 0.8632\n","Epoch [1/3], Step [3400/15623], Loss: 0.8666\n","Epoch [1/3], Step [3500/15623], Loss: 0.8637\n","Epoch [1/3], Step [3600/15623], Loss: 0.8662\n","Epoch [1/3], Step [3700/15623], Loss: 0.8661\n","Epoch [1/3], Step [3800/15623], Loss: 0.8678\n","Epoch [1/3], Step [3900/15623], Loss: 0.8639\n","Epoch [1/3], Step [4000/15623], Loss: 0.8606\n","Epoch [1/3], Step [4100/15623], Loss: 0.8622\n","Epoch [1/3], Step [4200/15623], Loss: 0.8616\n","Epoch [1/3], Step [4300/15623], Loss: 0.8625\n","Epoch [1/3], Step [4400/15623], Loss: 0.8638\n","Epoch [1/3], Step [4500/15623], Loss: 0.8671\n","Epoch [1/3], Step [4600/15623], Loss: 0.8639\n","Epoch [1/3], Step [4700/15623], Loss: 0.8643\n","Epoch [1/3], Step [4800/15623], Loss: 0.8634\n","Epoch [1/3], Step [4900/15623], Loss: 0.8611\n","Epoch [1/3], Step [5000/15623], Loss: 0.8639\n","Epoch [1/3], Step [5100/15623], Loss: 0.8625\n","Epoch [1/3], Step [5200/15623], Loss: 0.8612\n","Epoch [1/3], Step [5300/15623], Loss: 0.8674\n","Epoch [1/3], Step [5400/15623], Loss: 0.8652\n","Epoch [1/3], Step [5500/15623], Loss: 0.8620\n","Epoch [1/3], Step [5600/15623], Loss: 0.8615\n","Epoch [1/3], Step [5700/15623], Loss: 0.8637\n","Epoch [1/3], Step [5800/15623], Loss: 0.8595\n","Epoch [1/3], Step [5900/15623], Loss: 0.8620\n","Epoch [1/3], Step [6000/15623], Loss: 0.8633\n","Epoch [1/3], Step [6100/15623], Loss: 0.8639\n","Epoch [1/3], Step [6200/15623], Loss: 0.8623\n","Epoch [1/3], Step [6300/15623], Loss: 0.8677\n","Epoch [1/3], Step [6400/15623], Loss: 0.8663\n","Epoch [1/3], Step [6500/15623], Loss: 0.8648\n","Epoch [1/3], Step [6600/15623], Loss: 0.8637\n","Epoch [1/3], Step [6700/15623], Loss: 0.8627\n","Epoch [1/3], Step [6800/15623], Loss: 0.8633\n","Epoch [1/3], Step [6900/15623], Loss: 0.8675\n","Epoch [1/3], Step [7000/15623], Loss: 0.8640\n","Epoch [1/3], Step [7100/15623], Loss: 0.8640\n","Epoch [1/3], Step [7200/15623], Loss: 0.8636\n","Epoch [1/3], Step [7300/15623], Loss: 0.8600\n","Epoch [1/3], Step [7400/15623], Loss: 0.8663\n","Epoch [1/3], Step [7500/15623], Loss: 0.8647\n","Epoch [1/3], Step [7600/15623], Loss: 0.8618\n","Epoch [1/3], Step [7700/15623], Loss: 0.8603\n","Epoch [1/3], Step [7800/15623], Loss: 0.8603\n","Epoch [1/3], Step [7900/15623], Loss: 0.8626\n","Epoch [1/3], Step [8000/15623], Loss: 0.8638\n","Epoch [1/3], Step [8100/15623], Loss: 0.8641\n","Epoch [1/3], Step [8200/15623], Loss: 0.8636\n","Epoch [1/3], Step [8300/15623], Loss: 0.8649\n","Epoch [1/3], Step [8400/15623], Loss: 0.8599\n","Epoch [1/3], Step [8500/15623], Loss: 0.8662\n","Epoch [1/3], Step [8600/15623], Loss: 0.8636\n","Epoch [1/3], Step [8700/15623], Loss: 0.8601\n","Epoch [1/3], Step [8800/15623], Loss: 0.8682\n","Epoch [1/3], Step [8900/15623], Loss: 0.8659\n","Epoch [1/3], Step [9000/15623], Loss: 0.8636\n","Epoch [1/3], Step [9100/15623], Loss: 0.8632\n","Epoch [1/3], Step [9200/15623], Loss: 0.8595\n","Epoch [1/3], Step [9300/15623], Loss: 0.8606\n","Epoch [1/3], Step [9400/15623], Loss: 0.8609\n","Epoch [1/3], Step [9500/15623], Loss: 0.8653\n","Epoch [1/3], Step [9600/15623], Loss: 0.8612\n","Epoch [1/3], Step [9700/15623], Loss: 0.8649\n","Epoch [1/3], Step [9800/15623], Loss: 0.8652\n","Epoch [1/3], Step [9900/15623], Loss: 0.8598\n","Epoch [1/3], Step [10000/15623], Loss: 0.8624\n","Epoch [1/3], Step [10100/15623], Loss: 0.8641\n","Epoch [1/3], Step [10200/15623], Loss: 0.8634\n","Epoch [1/3], Step [10300/15623], Loss: 0.8688\n","Epoch [1/3], Step [10400/15623], Loss: 0.8655\n","Epoch [1/3], Step [10500/15623], Loss: 0.8684\n","Epoch [1/3], Step [10600/15623], Loss: 0.8628\n","Epoch [1/3], Step [10700/15623], Loss: 0.8633\n","Epoch [1/3], Step [10800/15623], Loss: 0.8627\n","Epoch [1/3], Step [10900/15623], Loss: 0.8637\n","Epoch [1/3], Step [11000/15623], Loss: 0.8655\n","Epoch [1/3], Step [11100/15623], Loss: 0.8635\n","Epoch [1/3], Step [11200/15623], Loss: 0.8614\n","Epoch [1/3], Step [11300/15623], Loss: 0.8655\n","Epoch [1/3], Step [11400/15623], Loss: 0.8622\n","Epoch [1/3], Step [11500/15623], Loss: 0.8661\n","Epoch [1/3], Step [11600/15623], Loss: 0.8642\n","Epoch [1/3], Step [11700/15623], Loss: 0.8614\n","Epoch [1/3], Step [11800/15623], Loss: 0.8635\n","Epoch [1/3], Step [11900/15623], Loss: 0.8593\n","Epoch [1/3], Step [12000/15623], Loss: 0.8612\n","Epoch [1/3], Step [12100/15623], Loss: 0.8613\n","Epoch [1/3], Step [12200/15623], Loss: 0.8654\n","Epoch [1/3], Step [12300/15623], Loss: 0.8633\n","Epoch [1/3], Step [12400/15623], Loss: 0.8641\n","Epoch [1/3], Step [12500/15623], Loss: 0.8646\n","Epoch [1/3], Step [12600/15623], Loss: 0.8652\n","Epoch [1/3], Step [12700/15623], Loss: 0.8637\n","Epoch [1/3], Step [12800/15623], Loss: 0.8642\n","Epoch [1/3], Step [12900/15623], Loss: 0.8628\n","Epoch [1/3], Step [13000/15623], Loss: 0.8653\n","Epoch [1/3], Step [13100/15623], Loss: 0.8609\n","Epoch [1/3], Step [13200/15623], Loss: 0.8654\n","Epoch [1/3], Step [13300/15623], Loss: 0.8604\n","Epoch [1/3], Step [13400/15623], Loss: 0.8627\n","Epoch [1/3], Step [13500/15623], Loss: 0.8614\n","Epoch [1/3], Step [13600/15623], Loss: 0.8630\n","Epoch [1/3], Step [13700/15623], Loss: 0.8629\n","Epoch [1/3], Step [13800/15623], Loss: 0.8638\n","Epoch [1/3], Step [13900/15623], Loss: 0.8655\n","Epoch [1/3], Step [14000/15623], Loss: 0.8625\n","Epoch [1/3], Step [14100/15623], Loss: 0.8600\n","Epoch [1/3], Step [14200/15623], Loss: 0.8642\n","Epoch [1/3], Step [14300/15623], Loss: 0.8633\n","Epoch [1/3], Step [14400/15623], Loss: 0.8614\n","Epoch [1/3], Step [14500/15623], Loss: 0.8644\n","Epoch [1/3], Step [14600/15623], Loss: 0.8629\n","Epoch [1/3], Step [14700/15623], Loss: 0.8640\n","Epoch [1/3], Step [14800/15623], Loss: 0.8688\n","Epoch [1/3], Step [14900/15623], Loss: 0.8655\n","Epoch [1/3], Step [15000/15623], Loss: 0.8689\n","Epoch [1/3], Step [15100/15623], Loss: 0.8610\n","Epoch [1/3], Step [15200/15623], Loss: 0.8642\n","Epoch [1/3], Step [15300/15623], Loss: 0.8653\n","Epoch [1/3], Step [15400/15623], Loss: 0.8602\n","Epoch [1/3], Step [15500/15623], Loss: 0.8646\n","Epoch [1/3], Step [15600/15623], Loss: 0.8616\n","Epoch [2/3], Step [0/15623], Loss: 0.8747\n","Epoch [2/3], Step [100/15623], Loss: 0.8666\n","Epoch [2/3], Step [200/15623], Loss: 0.8634\n","Epoch [2/3], Step [300/15623], Loss: 0.8624\n","Epoch [2/3], Step [400/15623], Loss: 0.8615\n","Epoch [2/3], Step [500/15623], Loss: 0.8667\n","Epoch [2/3], Step [600/15623], Loss: 0.8608\n","Epoch [2/3], Step [700/15623], Loss: 0.8602\n","Epoch [2/3], Step [800/15623], Loss: 0.8650\n","Epoch [2/3], Step [900/15623], Loss: 0.8659\n","Epoch [2/3], Step [1000/15623], Loss: 0.8674\n","Epoch [2/3], Step [1100/15623], Loss: 0.8627\n","Epoch [2/3], Step [1200/15623], Loss: 0.8606\n","Epoch [2/3], Step [1300/15623], Loss: 0.8642\n","Epoch [2/3], Step [1400/15623], Loss: 0.8619\n","Epoch [2/3], Step [1500/15623], Loss: 0.8627\n","Epoch [2/3], Step [1600/15623], Loss: 0.8607\n","Epoch [2/3], Step [1700/15623], Loss: 0.8672\n","Epoch [2/3], Step [1800/15623], Loss: 0.8604\n","Epoch [2/3], Step [1900/15623], Loss: 0.8615\n","Epoch [2/3], Step [2000/15623], Loss: 0.8618\n","Epoch [2/3], Step [2100/15623], Loss: 0.8625\n","Epoch [2/3], Step [2200/15623], Loss: 0.8668\n","Epoch [2/3], Step [2300/15623], Loss: 0.8644\n","Epoch [2/3], Step [2400/15623], Loss: 0.8577\n","Epoch [2/3], Step [2500/15623], Loss: 0.8631\n","Epoch [2/3], Step [2600/15623], Loss: 0.8636\n","Epoch [2/3], Step [2700/15623], Loss: 0.8650\n","Epoch [2/3], Step [2800/15623], Loss: 0.8636\n","Epoch [2/3], Step [2900/15623], Loss: 0.8658\n","Epoch [2/3], Step [3000/15623], Loss: 0.8625\n","Epoch [2/3], Step [3100/15623], Loss: 0.8602\n","Epoch [2/3], Step [3200/15623], Loss: 0.8701\n","Epoch [2/3], Step [3300/15623], Loss: 0.8616\n","Epoch [2/3], Step [3400/15623], Loss: 0.8654\n","Epoch [2/3], Step [3500/15623], Loss: 0.8639\n","Epoch [2/3], Step [3600/15623], Loss: 0.8641\n","Epoch [2/3], Step [3700/15623], Loss: 0.8658\n","Epoch [2/3], Step [3800/15623], Loss: 0.8650\n","Epoch [2/3], Step [3900/15623], Loss: 0.8627\n","Epoch [2/3], Step [4000/15623], Loss: 0.8640\n","Epoch [2/3], Step [4100/15623], Loss: 0.8667\n","Epoch [2/3], Step [4200/15623], Loss: 0.8635\n","Epoch [2/3], Step [4300/15623], Loss: 0.8630\n","Epoch [2/3], Step [4400/15623], Loss: 0.8649\n","Epoch [2/3], Step [4500/15623], Loss: 0.8630\n","Epoch [2/3], Step [4600/15623], Loss: 0.8612\n","Epoch [2/3], Step [4700/15623], Loss: 0.8676\n","Epoch [2/3], Step [4800/15623], Loss: 0.8595\n","Epoch [2/3], Step [4900/15623], Loss: 0.8628\n","Epoch [2/3], Step [5000/15623], Loss: 0.8617\n","Epoch [2/3], Step [5100/15623], Loss: 0.8613\n","Epoch [2/3], Step [5200/15623], Loss: 0.8659\n","Epoch [2/3], Step [5300/15623], Loss: 0.8637\n","Epoch [2/3], Step [5400/15623], Loss: 0.8606\n","Epoch [2/3], Step [5500/15623], Loss: 0.8617\n","Epoch [2/3], Step [5600/15623], Loss: 0.8631\n","Epoch [2/3], Step [5700/15623], Loss: 0.8631\n","Epoch [2/3], Step [5800/15623], Loss: 0.8626\n","Epoch [2/3], Step [5900/15623], Loss: 0.8616\n","Epoch [2/3], Step [6000/15623], Loss: 0.8661\n","Epoch [2/3], Step [6100/15623], Loss: 0.8654\n","Epoch [2/3], Step [6200/15623], Loss: 0.8631\n","Epoch [2/3], Step [6300/15623], Loss: 0.8680\n","Epoch [2/3], Step [6400/15623], Loss: 0.8619\n","Epoch [2/3], Step [6500/15623], Loss: 0.8614\n","Epoch [2/3], Step [6600/15623], Loss: 0.8628\n","Epoch [2/3], Step [6700/15623], Loss: 0.8632\n","Epoch [2/3], Step [6800/15623], Loss: 0.8623\n","Epoch [2/3], Step [6900/15623], Loss: 0.8612\n","Epoch [2/3], Step [7000/15623], Loss: 0.8638\n","Epoch [2/3], Step [7100/15623], Loss: 0.8643\n","Epoch [2/3], Step [7200/15623], Loss: 0.8631\n","Epoch [2/3], Step [7300/15623], Loss: 0.8617\n","Epoch [2/3], Step [7400/15623], Loss: 0.8656\n","Epoch [2/3], Step [7500/15623], Loss: 0.8615\n","Epoch [2/3], Step [7600/15623], Loss: 0.8601\n","Epoch [2/3], Step [7700/15623], Loss: 0.8641\n","Epoch [2/3], Step [7800/15623], Loss: 0.8656\n","Epoch [2/3], Step [7900/15623], Loss: 0.8626\n","Epoch [2/3], Step [8000/15623], Loss: 0.8629\n","Epoch [2/3], Step [8100/15623], Loss: 0.8655\n","Epoch [2/3], Step [8200/15623], Loss: 0.8613\n","Epoch [2/3], Step [8300/15623], Loss: 0.8631\n","Epoch [2/3], Step [8400/15623], Loss: 0.8673\n","Epoch [2/3], Step [8500/15623], Loss: 0.8631\n","Epoch [2/3], Step [8600/15623], Loss: 0.8653\n","Epoch [2/3], Step [8700/15623], Loss: 0.8649\n","Epoch [2/3], Step [8800/15623], Loss: 0.8621\n","Epoch [2/3], Step [8900/15623], Loss: 0.8618\n","Epoch [2/3], Step [9000/15623], Loss: 0.8637\n","Epoch [2/3], Step [9100/15623], Loss: 0.8629\n","Epoch [2/3], Step [9200/15623], Loss: 0.8646\n","Epoch [2/3], Step [9300/15623], Loss: 0.8663\n","Epoch [2/3], Step [9400/15623], Loss: 0.8634\n","Epoch [2/3], Step [9500/15623], Loss: 0.8605\n","Epoch [2/3], Step [9600/15623], Loss: 0.8650\n","Epoch [2/3], Step [9700/15623], Loss: 0.8646\n","Epoch [2/3], Step [9800/15623], Loss: 0.8611\n","Epoch [2/3], Step [9900/15623], Loss: 0.8605\n","Epoch [2/3], Step [10000/15623], Loss: 0.8612\n","Epoch [2/3], Step [10100/15623], Loss: 0.8601\n","Epoch [2/3], Step [10200/15623], Loss: 0.8578\n","Epoch [2/3], Step [10300/15623], Loss: 0.8647\n","Epoch [2/3], Step [10400/15623], Loss: 0.8605\n","Epoch [2/3], Step [10500/15623], Loss: 0.8610\n","Epoch [2/3], Step [10600/15623], Loss: 0.8618\n","Epoch [2/3], Step [10700/15623], Loss: 0.8591\n","Epoch [2/3], Step [10800/15623], Loss: 0.8637\n","Epoch [2/3], Step [10900/15623], Loss: 0.8628\n","Epoch [2/3], Step [11000/15623], Loss: 0.8605\n","Epoch [2/3], Step [11100/15623], Loss: 0.8622\n","Epoch [2/3], Step [11200/15623], Loss: 0.8654\n","Epoch [2/3], Step [11300/15623], Loss: 0.8641\n","Epoch [2/3], Step [11400/15623], Loss: 0.8623\n","Epoch [2/3], Step [11500/15623], Loss: 0.8609\n","Epoch [2/3], Step [11600/15623], Loss: 0.8599\n","Epoch [2/3], Step [11700/15623], Loss: 0.8638\n","Epoch [2/3], Step [11800/15623], Loss: 0.8614\n","Epoch [2/3], Step [11900/15623], Loss: 0.8634\n","Epoch [2/3], Step [12000/15623], Loss: 0.8630\n","Epoch [2/3], Step [12100/15623], Loss: 0.8624\n","Epoch [2/3], Step [12200/15623], Loss: 0.8608\n","Epoch [2/3], Step [12300/15623], Loss: 0.8649\n","Epoch [2/3], Step [12400/15623], Loss: 0.8622\n","Epoch [2/3], Step [12500/15623], Loss: 0.8658\n","Epoch [2/3], Step [12600/15623], Loss: 0.8653\n","Epoch [2/3], Step [12700/15623], Loss: 0.8659\n","Epoch [2/3], Step [12800/15623], Loss: 0.8624\n","Epoch [2/3], Step [12900/15623], Loss: 0.8646\n","Epoch [2/3], Step [13000/15623], Loss: 0.8682\n","Epoch [2/3], Step [13100/15623], Loss: 0.8664\n","Epoch [2/3], Step [13200/15623], Loss: 0.8611\n","Epoch [2/3], Step [13300/15623], Loss: 0.8649\n","Epoch [2/3], Step [13400/15623], Loss: 0.8689\n","Epoch [2/3], Step [13500/15623], Loss: 0.8593\n","Epoch [2/3], Step [13600/15623], Loss: 0.8640\n","Epoch [2/3], Step [13700/15623], Loss: 0.8595\n","Epoch [2/3], Step [13800/15623], Loss: 0.8595\n","Epoch [2/3], Step [13900/15623], Loss: 0.8606\n","Epoch [2/3], Step [14000/15623], Loss: 0.8596\n","Epoch [2/3], Step [14100/15623], Loss: 0.8619\n","Epoch [2/3], Step [14200/15623], Loss: 0.8651\n","Epoch [2/3], Step [14300/15623], Loss: 0.8607\n","Epoch [2/3], Step [14400/15623], Loss: 0.8696\n","Epoch [2/3], Step [14500/15623], Loss: 0.8653\n","Epoch [2/3], Step [14600/15623], Loss: 0.8625\n","Epoch [2/3], Step [14700/15623], Loss: 0.8643\n","Epoch [2/3], Step [14800/15623], Loss: 0.8605\n","Epoch [2/3], Step [14900/15623], Loss: 0.8629\n","Epoch [2/3], Step [15000/15623], Loss: 0.8595\n","Epoch [2/3], Step [15100/15623], Loss: 0.8620\n","Epoch [2/3], Step [15200/15623], Loss: 0.8629\n","Epoch [2/3], Step [15300/15623], Loss: 0.8683\n","Epoch [2/3], Step [15400/15623], Loss: 0.8617\n","Epoch [2/3], Step [15500/15623], Loss: 0.8647\n","Epoch [2/3], Step [15600/15623], Loss: 0.8599\n","Epoch [3/3], Step [0/15623], Loss: 0.8684\n","Epoch [3/3], Step [100/15623], Loss: 0.8662\n","Epoch [3/3], Step [200/15623], Loss: 0.8608\n","Epoch [3/3], Step [300/15623], Loss: 0.8659\n","Epoch [3/3], Step [400/15623], Loss: 0.8643\n","Epoch [3/3], Step [500/15623], Loss: 0.8653\n","Epoch [3/3], Step [600/15623], Loss: 0.8593\n","Epoch [3/3], Step [700/15623], Loss: 0.8624\n","Epoch [3/3], Step [800/15623], Loss: 0.8622\n","Epoch [3/3], Step [900/15623], Loss: 0.8631\n","Epoch [3/3], Step [1000/15623], Loss: 0.8651\n","Epoch [3/3], Step [1100/15623], Loss: 0.8612\n","Epoch [3/3], Step [1200/15623], Loss: 0.8641\n","Epoch [3/3], Step [1300/15623], Loss: 0.8650\n","Epoch [3/3], Step [1400/15623], Loss: 0.8646\n","Epoch [3/3], Step [1500/15623], Loss: 0.8602\n","Epoch [3/3], Step [1600/15623], Loss: 0.8597\n","Epoch [3/3], Step [1700/15623], Loss: 0.8634\n","Epoch [3/3], Step [1800/15623], Loss: 0.8643\n","Epoch [3/3], Step [1900/15623], Loss: 0.8664\n","Epoch [3/3], Step [2000/15623], Loss: 0.8640\n","Epoch [3/3], Step [2100/15623], Loss: 0.8642\n","Epoch [3/3], Step [2200/15623], Loss: 0.8614\n","Epoch [3/3], Step [2300/15623], Loss: 0.8633\n","Epoch [3/3], Step [2400/15623], Loss: 0.8667\n","Epoch [3/3], Step [2500/15623], Loss: 0.8604\n","Epoch [3/3], Step [2600/15623], Loss: 0.8639\n","Epoch [3/3], Step [2700/15623], Loss: 0.8607\n","Epoch [3/3], Step [2800/15623], Loss: 0.8647\n","Epoch [3/3], Step [2900/15623], Loss: 0.8640\n","Epoch [3/3], Step [3000/15623], Loss: 0.8625\n","Epoch [3/3], Step [3100/15623], Loss: 0.8599\n","Epoch [3/3], Step [3200/15623], Loss: 0.8675\n","Epoch [3/3], Step [3300/15623], Loss: 0.8653\n","Epoch [3/3], Step [3400/15623], Loss: 0.8646\n","Epoch [3/3], Step [3500/15623], Loss: 0.8672\n","Epoch [3/3], Step [3600/15623], Loss: 0.8600\n","Epoch [3/3], Step [3700/15623], Loss: 0.8658\n","Epoch [3/3], Step [3800/15623], Loss: 0.8641\n","Epoch [3/3], Step [3900/15623], Loss: 0.8639\n","Epoch [3/3], Step [4000/15623], Loss: 0.8631\n","Epoch [3/3], Step [4100/15623], Loss: 0.8624\n","Epoch [3/3], Step [4200/15623], Loss: 0.8613\n","Epoch [3/3], Step [4300/15623], Loss: 0.8615\n","Epoch [3/3], Step [4400/15623], Loss: 0.8611\n","Epoch [3/3], Step [4500/15623], Loss: 0.8641\n","Epoch [3/3], Step [4600/15623], Loss: 0.8673\n","Epoch [3/3], Step [4700/15623], Loss: 0.8628\n","Epoch [3/3], Step [4800/15623], Loss: 0.8629\n","Epoch [3/3], Step [4900/15623], Loss: 0.8624\n","Epoch [3/3], Step [5000/15623], Loss: 0.8672\n","Epoch [3/3], Step [5100/15623], Loss: 0.8662\n","Epoch [3/3], Step [5200/15623], Loss: 0.8615\n","Epoch [3/3], Step [5300/15623], Loss: 0.8613\n","Epoch [3/3], Step [5400/15623], Loss: 0.8619\n","Epoch [3/3], Step [5500/15623], Loss: 0.8606\n","Epoch [3/3], Step [5600/15623], Loss: 0.8630\n","Epoch [3/3], Step [5700/15623], Loss: 0.8648\n","Epoch [3/3], Step [5800/15623], Loss: 0.8619\n","Epoch [3/3], Step [5900/15623], Loss: 0.8626\n","Epoch [3/3], Step [6000/15623], Loss: 0.8618\n","Epoch [3/3], Step [6100/15623], Loss: 0.8633\n","Epoch [3/3], Step [6200/15623], Loss: 0.8620\n","Epoch [3/3], Step [6300/15623], Loss: 0.8596\n","Epoch [3/3], Step [6400/15623], Loss: 0.8661\n","Epoch [3/3], Step [6500/15623], Loss: 0.8650\n","Epoch [3/3], Step [6600/15623], Loss: 0.8603\n","Epoch [3/3], Step [6700/15623], Loss: 0.8573\n","Epoch [3/3], Step [6800/15623], Loss: 0.8613\n","Epoch [3/3], Step [6900/15623], Loss: 0.8627\n","Epoch [3/3], Step [7000/15623], Loss: 0.8603\n","Epoch [3/3], Step [7100/15623], Loss: 0.8670\n","Epoch [3/3], Step [7200/15623], Loss: 0.8635\n","Epoch [3/3], Step [7300/15623], Loss: 0.8594\n","Epoch [3/3], Step [7400/15623], Loss: 0.8642\n","Epoch [3/3], Step [7500/15623], Loss: 0.8615\n","Epoch [3/3], Step [7600/15623], Loss: 0.8644\n","Epoch [3/3], Step [7700/15623], Loss: 0.8599\n","Epoch [3/3], Step [7800/15623], Loss: 0.8622\n","Epoch [3/3], Step [7900/15623], Loss: 0.8639\n","Epoch [3/3], Step [8000/15623], Loss: 0.8621\n","Epoch [3/3], Step [8100/15623], Loss: 0.8663\n","Epoch [3/3], Step [8200/15623], Loss: 0.8641\n","Epoch [3/3], Step [8300/15623], Loss: 0.8663\n","Epoch [3/3], Step [8400/15623], Loss: 0.8636\n","Epoch [3/3], Step [8500/15623], Loss: 0.8627\n","Epoch [3/3], Step [8600/15623], Loss: 0.8623\n","Epoch [3/3], Step [8700/15623], Loss: 0.8642\n","Epoch [3/3], Step [8800/15623], Loss: 0.8617\n","Epoch [3/3], Step [8900/15623], Loss: 0.8646\n","Epoch [3/3], Step [9000/15623], Loss: 0.8672\n","Epoch [3/3], Step [9100/15623], Loss: 0.8637\n","Epoch [3/3], Step [9200/15623], Loss: 0.8601\n","Epoch [3/3], Step [9300/15623], Loss: 0.8628\n","Epoch [3/3], Step [9400/15623], Loss: 0.8643\n","Epoch [3/3], Step [9500/15623], Loss: 0.8604\n","Epoch [3/3], Step [9600/15623], Loss: 0.8638\n","Epoch [3/3], Step [9700/15623], Loss: 0.8646\n","Epoch [3/3], Step [9800/15623], Loss: 0.8631\n","Epoch [3/3], Step [9900/15623], Loss: 0.8597\n","Epoch [3/3], Step [10000/15623], Loss: 0.8691\n","Epoch [3/3], Step [10100/15623], Loss: 0.8640\n","Epoch [3/3], Step [10200/15623], Loss: 0.8628\n","Epoch [3/3], Step [10300/15623], Loss: 0.8637\n","Epoch [3/3], Step [10400/15623], Loss: 0.8652\n","Epoch [3/3], Step [10500/15623], Loss: 0.8620\n","Epoch [3/3], Step [10600/15623], Loss: 0.8650\n","Epoch [3/3], Step [10700/15623], Loss: 0.8652\n","Epoch [3/3], Step [10800/15623], Loss: 0.8613\n","Epoch [3/3], Step [10900/15623], Loss: 0.8630\n","Epoch [3/3], Step [11000/15623], Loss: 0.8598\n","Epoch [3/3], Step [11100/15623], Loss: 0.8640\n","Epoch [3/3], Step [11200/15623], Loss: 0.8666\n","Epoch [3/3], Step [11300/15623], Loss: 0.8634\n","Epoch [3/3], Step [11400/15623], Loss: 0.8668\n","Epoch [3/3], Step [11500/15623], Loss: 0.8648\n","Epoch [3/3], Step [11600/15623], Loss: 0.8642\n","Epoch [3/3], Step [11700/15623], Loss: 0.8650\n","Epoch [3/3], Step [11800/15623], Loss: 0.8651\n","Epoch [3/3], Step [11900/15623], Loss: 0.8659\n","Epoch [3/3], Step [12000/15623], Loss: 0.8638\n","Epoch [3/3], Step [12100/15623], Loss: 0.8635\n","Epoch [3/3], Step [12200/15623], Loss: 0.8628\n","Epoch [3/3], Step [12300/15623], Loss: 0.8660\n","Epoch [3/3], Step [12400/15623], Loss: 0.8668\n","Epoch [3/3], Step [12500/15623], Loss: 0.8608\n","Epoch [3/3], Step [12600/15623], Loss: 0.8611\n","Epoch [3/3], Step [12700/15623], Loss: 0.8663\n","Epoch [3/3], Step [12800/15623], Loss: 0.8634\n","Epoch [3/3], Step [12900/15623], Loss: 0.8648\n","Epoch [3/3], Step [13000/15623], Loss: 0.8618\n","Epoch [3/3], Step [13100/15623], Loss: 0.8685\n","Epoch [3/3], Step [13200/15623], Loss: 0.8629\n","Epoch [3/3], Step [13300/15623], Loss: 0.8639\n","Epoch [3/3], Step [13400/15623], Loss: 0.8634\n","Epoch [3/3], Step [13500/15623], Loss: 0.8641\n","Epoch [3/3], Step [13600/15623], Loss: 0.8626\n","Epoch [3/3], Step [13700/15623], Loss: 0.8601\n","Epoch [3/3], Step [13800/15623], Loss: 0.8612\n","Epoch [3/3], Step [13900/15623], Loss: 0.8634\n","Epoch [3/3], Step [14000/15623], Loss: 0.8666\n","Epoch [3/3], Step [14100/15623], Loss: 0.8615\n","Epoch [3/3], Step [14200/15623], Loss: 0.8647\n","Epoch [3/3], Step [14300/15623], Loss: 0.8573\n","Epoch [3/3], Step [14400/15623], Loss: 0.8660\n","Epoch [3/3], Step [14500/15623], Loss: 0.8641\n","Epoch [3/3], Step [14600/15623], Loss: 0.8616\n","Epoch [3/3], Step [14700/15623], Loss: 0.8604\n","Epoch [3/3], Step [14800/15623], Loss: 0.8624\n","Epoch [3/3], Step [14900/15623], Loss: 0.8639\n","Epoch [3/3], Step [15000/15623], Loss: 0.8630\n","Epoch [3/3], Step [15100/15623], Loss: 0.8649\n","Epoch [3/3], Step [15200/15623], Loss: 0.8655\n","Epoch [3/3], Step [15300/15623], Loss: 0.8642\n","Epoch [3/3], Step [15400/15623], Loss: 0.8649\n","Epoch [3/3], Step [15500/15623], Loss: 0.8634\n","Epoch [3/3], Step [15600/15623], Loss: 0.8673\n"]}]},{"cell_type":"code","source":["# prompt: Load model from rnnlm_epoch1.pth\n","\n","# Load the model's state dictionary\n","model.load_state_dict(torch.load('rnnlm_epoch1.pth'))\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","print(\"Model loaded successfully from rnnlm_epoch1.pth\")\n","\n"],"metadata":{"id":"f98UUCTNry8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = {x:i for i,x in enumerate(set(text_data))}"],"metadata":{"id":"l6Sojht7yhYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Generate 10 next tokens from the context string 'AAAA'\n","\n","def generate_next_tokens(model, tokenizer, context_string, num_tokens=10):\n","  \"\"\"\n","  Generates the next tokens based on the provided context string using the trained RNNLM model.\n","\n","  Args:\n","      model: The trained RNNLM model.\n","      context_string: The context string (e.g., \"AAAA\").\n","      num_tokens: The number of tokens to generate.\n","\n","  Returns:\n","      A string containing the generated tokens.\n","  \"\"\"\n","  itos = {i: x for x, i in tokenizer.items()}\n","\n","  # Convert the context string to tokens\n","  context_tokens = [tokenizer[char] for char in context_string]\n","  context_tensor = torch.tensor([context_tokens], dtype=torch.long).to(DEVICE)\n","\n","  # Initialize hidden state\n","  hidden = None\n","\n","  generated_tokens = []\n","\n","  # Generate next tokens\n","  with torch.no_grad():\n","    #first lets use context\n","    for token in context_tensor[0]:\n","      output, hidden = model(token.unsqueeze(0).unsqueeze(0),hidden)\n","    for _ in range(num_tokens):\n","        output, hidden = model(token.unsqueeze(0).unsqueeze(0), hidden)\n","        probabilities = torch.softmax(output[0, -1, :], dim=0)\n","        # print probability vector with label from tokenizer\n","        for i, prob in enumerate(probabilities):\n","            print(f\"{itos[i]}: {prob.item():.4f}\")\n","\n","        next_token_idx = torch.multinomial(probabilities, num_samples=1).item()\n","        generated_tokens.append(itos[next_token_idx])\n","\n","        print(f\"I got {itos[next_token_idx]}. Now string is {context_string + ''.join(generated_tokens)}\")\n","\n","        token = torch.tensor(next_token_idx).to(DEVICE)\n","  return \"\".join(generated_tokens)\n","\n","# Example usage:\n","context_string = \"AAAA\"\n","next_tokens = generate_next_tokens(model, tokenizer, context_string)\n","print(f\"Context: '{context_string}'\")\n","print(f\"Next tokens: '{next_tokens}'\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkOBr2GyyJLD","executionInfo":{"status":"ok","timestamp":1738637222908,"user_tz":-420,"elapsed":824,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"6644d08c-2796-4a93-afe3-7bbd35fa4faa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["B: 0.6624\n","C: 0.1674\n","D: 0.0001\n","A: 0.1700\n","E: 0.0001\n","I got B. Now string is AAAAB\n","B: 0.0001\n","C: 0.4974\n","D: 0.0000\n","A: 0.5020\n","E: 0.0004\n","I got A. Now string is AAAABA\n","B: 0.0048\n","C: 0.5080\n","D: 0.0000\n","A: 0.4871\n","E: 0.0001\n","I got A. Now string is AAAABAA\n","B: 0.5574\n","C: 0.2227\n","D: 0.0001\n","A: 0.2198\n","E: 0.0000\n","I got B. Now string is AAAABAAB\n","B: 0.0000\n","C: 0.5003\n","D: 0.0000\n","A: 0.4994\n","E: 0.0003\n","I got C. Now string is AAAABAABC\n","B: 0.0000\n","C: 0.4852\n","D: 0.0002\n","A: 0.5125\n","E: 0.0021\n","I got C. Now string is AAAABAABCC\n","B: 0.0000\n","C: 0.4797\n","D: 0.0011\n","A: 0.5173\n","E: 0.0019\n","I got C. Now string is AAAABAABCCC\n","B: 0.0000\n","C: 0.4827\n","D: 0.0040\n","A: 0.5121\n","E: 0.0012\n","I got C. Now string is AAAABAABCCCC\n","B: 0.0001\n","C: 0.3063\n","D: 0.3752\n","A: 0.3169\n","E: 0.0016\n","I got D. Now string is AAAABAABCCCCD\n","B: 0.0000\n","C: 0.5079\n","D: 0.0000\n","A: 0.4920\n","E: 0.0001\n","I got C. Now string is AAAABAABCCCCDC\n","Context: 'AAAA'\n","Next tokens: 'BAABCCCCDC'\n"]}]},{"cell_type":"code","source":["tokenizer"],"metadata":{"id":"sZ0rmpKXzNBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_data[0:100]"],"metadata":{"id":"kPgKesqjzVtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kroc16-lzaSs"},"execution_count":null,"outputs":[]}]}