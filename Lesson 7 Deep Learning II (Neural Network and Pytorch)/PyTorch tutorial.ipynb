{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XPzljOtUg9kx"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n"]},{"cell_type":"code","source":["!pip install pythainlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6LCh54Wo29E","executionInfo":{"status":"ok","timestamp":1740114850365,"user_tz":-420,"elapsed":1818,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"8fd8daf5-5490-4e81-eedb-3d9aca4af60b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pythainlp in /usr/local/lib/python3.11/dist-packages (5.0.5)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from pythainlp) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pythainlp) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pythainlp) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pythainlp) (2025.1.31)\n"]}]},{"cell_type":"markdown","source":["# 1) Tokenization\n","The goal of tokenization here is to\n","\n","1) Convert string to a list of token strings -- so we need a word segmenter/tokenizer here\n","\n","2) Convert a list of token strings to a list of token indices -- so we need\n","- a dictionary to keep index --> word\n","- a dictionary to keep word --> index\n","\n","3) Pad or truncate to the same length"],"metadata":{"id":"YAoNP4_vhn7n"}},{"cell_type":"code","source":["import pythainlp\n","class Tokenizer:\n","\n","    def __init__(self, seq_length):\n","        self.word2idx = {'<PAD>':0}\n","        self.idx2word = {0:'<PAD>'}\n","        self.UNK = None\n","        self.word_segmenter = pythainlp.word_tokenize\n","        self.seq_length = seq_length\n","\n","    def tokenize_training_set(self, text_list):\n","        training_set = []\n","        for text in text_list:\n","            tokens = self.word_segmenter(text)\n","            for token in tokens:\n","                if token not in self.word2idx:\n","                    index = len(self.word2idx)\n","                    self.word2idx[token] = index\n","                    self.idx2word[index] = token\n","            tokens = [self.word2idx[token] for token in tokens]\n","            tokens = self.pad_or_truncate(tokens)\n","            training_set.append(tokens)\n","        self.UNK = len(self.word2idx)\n","        self.word2idx['<UNK>'] = self.UNK\n","        self.idx2word[self.UNK] = '<UNK>'\n","        return training_set\n","\n","    def pad_or_truncate(self, index_list):\n","        if len(index_list) >= self.seq_length:\n","            return index_list[:self.seq_length]\n","        return index_list + [self.word2idx['<PAD>']] * (self.seq_length - len(index_list))\n","\n","    def tokenize_dataset(self, text_list):\n","        list_list_tokens = [self.word_segmenter(text) for text in text_list]\n","        list_list_indices = [[self.word2idx.get(token, self.UNK) for token in tokens] for tokens in list_list_tokens]\n","        list_list_indices = [self.pad_or_truncate(indices) for indices in list_list_indices]\n","        return list_list_indices\n","\n","    def tokenize_sentence(self, text):\n","        list_indices = [self.word2idx.get(token, self.UNK) for token in self.word_segmenter(text)]\n","        list_indices = self.pad_or_truncate(list_indices)\n","        return list_indices\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UXq6U2xhAWs","executionInfo":{"status":"error","timestamp":1740125685648,"user_tz":-420,"elapsed":310,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"c2912b09-e7c6-4d26-dc52-b18e043394a6"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'pythainlp'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4dbfc56f906c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpythainlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pythainlp'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["# 2) Create DataSet container and DataLoader\n","\n","We subclass `DataSet` class to store a list of tokens and labels. The goal is to\n","\n","1) keep X and Y in one place.\n","\n","2) Make sure that train dev and test sets\n","\n","We must implement three methods: `__init__`, `__len__`, and `__getitem__`.\n","\n","We feed then the dataset to `DataLoader`, which helps us batch data for training (or inference)\n"],"metadata":{"id":"DO6Sx5l0iUWp"}},{"cell_type":"code","source":["class TextDataset(Dataset):\n","\n","    def __init__(self, X, Y, tokenizer):\n","        # count indices that are not padding\n","        self.lengths = [sum(1 for x in indices if x != tokenizer.word2idx['<PAD>']) for indices in X]\n","        self.X = X\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.X)  # Number of samples in dataset\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.lengths[idx]), torch.tensor(self.Y[idx], dtype=torch.long)"],"metadata":{"id":"8OGsw-WplaE2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3) Set up the model\n","\n","We will subclass `torch.nn.Module` because it works well with training functions provided by Torch. We will implement two methods `__init__` and `forward`\n"],"metadata":{"id":"irSMvBBxpuNR"}},{"cell_type":"code","source":["class DeepAveragingNetwork(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n","        super(DeepAveragingNetwork, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n","        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x, lengths):\n","        # x (batch, seq_len)\n","        # lengths (batch)\n","        embeds = self.embedding(x)  # (batch, seq_len, embed_dim)\n","        sum_embeds = torch.sum(embeds, dim=1)  # (batch, embed_dim)\n","        avg_embeds = sum_embeds / lengths.unsqueeze(1).float()\n","        hidden = self.relu(self.fc1(avg_embeds))\n","        output = self.softmax(self.fc2(hidden)) #(batch, num class)\n","        return output\n"],"metadata":{"id":"FogpvodQpuWf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4) Load up the data"],"metadata":{"id":"SBkvfrJb1Ivk"}},{"cell_type":"code","source":["# Training set\n","train_set = [\n","    (\"มีสาขาที่ใกล้ที่สุดอยู่ที่ไหน?\", \"Question\"),\n","    (\"ร้านปิดกี่โมง?\", \"Question\"),\n","    (\"ส่งของใช้เวลากี่วัน?\", \"Question\"),\n","    (\"มีโปรโมชั่นอะไรบ้าง?\", \"Question\"),\n","    (\"ช่วยโอนสายไปที่เจ้าหน้าที่ได้ไหม?\", \"Ask for agent\"),\n","    (\"ขอคุยกับพนักงานหน่อย\", \"Ask for agent\"),\n","    (\"มีแอดมินตอบไหม?\", \"Ask for agent\"),\n","    (\"ต้องการคุยกับฝ่ายบริการลูกค้า\", \"Ask for agent\"),\n","    (\"สินค้าส่งมาผิด!\", \"Complaint\"),\n","    (\"ได้รับของเสียหาย\", \"Complaint\"),\n","    (\"พัสดุยังไม่มาส่งเลย\", \"Complaint\"),\n","    (\"ทำไมยังไม่ได้รับคำตอบ?\", \"Complaint\"),\n","    (\"ของที่ส่งมาขาดชิ้นส่วน\", \"Complaint\"),\n","    (\"พนักงานพูดจาไม่สุภาพ\", \"Complaint\"),\n","    (\"ระบบใช้งานไม่ได้\", \"Complaint\"),\n","]\n","\n","# Development set\n","dev_set = [\n","    (\"มีบริการเก็บเงินปลายทางไหม?\", \"Question\"),\n","    (\"สามารถเปลี่ยนที่อยู่จัดส่งได้หรือเปล่า?\", \"Question\"),\n","    (\"สินค้านี้มีสีอื่นไหม?\", \"Question\"),\n","    (\"ค่าจัดส่งเท่าไหร่?\", \"Question\"),\n","    (\"ขอโอนสายไปฝ่ายบัญชีได้ไหม?\", \"Ask for agent\"),\n","    (\"มีใครช่วยตอบคำถามได้ไหม?\", \"Ask for agent\"),\n","    (\"ช่วยหาคนที่รับผิดชอบให้หน่อย\", \"Ask for agent\"),\n","    (\"ต้องการพูดคุยกับฝ่ายเทคนิค\", \"Ask for agent\"),\n","    (\"ทำไมของที่ส่งมาใช้ไม่ได้?\", \"Complaint\"),\n","    (\"ได้รับของไม่ครบตามที่สั่ง\", \"Complaint\"),\n","    (\"แอปเด้งตลอดเวลา ใช้ไม่ได้เลย\", \"Complaint\"),\n","    (\"พนักงานไม่ช่วยแก้ปัญหาให้\", \"Complaint\"),\n","    (\"ระบบบอกว่าส่งแล้วแต่ยังไม่ได้รับของ\", \"Complaint\"),\n","    (\"ของที่ได้รับไม่ตรงกับที่โฆษณา\", \"Complaint\"),\n","    (\"ช่องทางติดต่อยากมาก\", \"Complaint\"),\n","]\n","\n","# Test set\n","test_set = [\n","    (\"สามารถยกเลิกคำสั่งซื้อได้ไหม?\", \"Question\"),\n","    (\"วิธีขอคืนเงินต้องทำอย่างไร?\", \"Question\"),\n","    (\"ใช้โค้ดส่วนลดอย่างไร?\", \"Question\"),\n","    (\"เมื่อไหร่สินค้าจะกลับมาในสต็อก?\", \"Question\"),\n","    (\"ช่วยโอนสายไปที่ผู้จัดการได้ไหม?\", \"Ask for agent\"),\n","    (\"ต้องการให้เจ้าหน้าที่โทรกลับ\", \"Ask for agent\"),\n","    (\"มีคนคอยช่วยเหลืออยู่ไหม?\", \"Ask for agent\"),\n","    (\"ช่วยติดต่อฝ่ายสนับสนุนให้หน่อย\", \"Ask for agent\"),\n","    (\"ส่งของผิดที่\", \"Complaint\"),\n","    (\"ของที่ได้รับมีรอยขีดข่วน\", \"Complaint\"),\n","    (\"ทำไมยังไม่มีการอัปเดตสถานะพัสดุ?\", \"Complaint\"),\n","    (\"ขอคืนเงินแต่ยังไม่ได้รับเงิน\", \"Complaint\"),\n","    (\"ได้รับสินค้าหมดอายุ\", \"Complaint\"),\n","    (\"แพ็กเกจของฉีกขาด\", \"Complaint\"),\n","    (\"พนักงานตอบช้ามาก\", \"Complaint\"),\n","]\n","\n","tokenizer = Tokenizer(seq_length=10)\n","tokenized_train_set = tokenizer.tokenize_training_set([text for text, label in train_set])\n","tokenized_dev_set = tokenizer.tokenize_dataset([text for text, label in dev_set])\n","tokenized_test_set = tokenizer.tokenize_dataset([text for text, label in test_set])\n","\n","label_dict = {'Question': 0, 'Ask for agent': 1, 'Complaint': 2}\n","Y_train = [label_dict[label] for text, label in train_set]\n","Y_dev = [label_dict[label] for text, label in dev_set]\n","Y_test = [label_dict[label] for text, label in test_set]\n","\n","train_dataset = TextDataset(tokenized_train_set, Y_train, tokenizer)\n","dev_dataset = TextDataset(tokenized_dev_set, Y_dev, tokenizer)\n","test_dataset = TextDataset(tokenized_test_set, Y_test, tokenizer)\n"],"metadata":{"id":"iQbI7w3C1kCV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5) Train"],"metadata":{"id":"HqH9hdcz1tqD"}},{"cell_type":"code","source":["# Model parameters\n","VOCAB_SIZE = len(tokenizer.word2idx)\n","EMBED_DIM = 50\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 3\n","\n","# Instantiate model\n","model = DeepAveragingNetwork(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n"],"metadata":{"id":"B6KPj8GI1I43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Train the model for 5 epochs and evaluate on dev_dataset every epoch\n","\n","# Training loop\n","EPOCHS = 5\n","BATCH_SIZE = 2\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    for batch in train_dataloader:\n","        X, lengths, Y = batch\n","        optimizer.zero_grad()\n","        predictions = model(X, lengths)\n","        loss = criterion(predictions, Y)\n","        loss.backward()\n","        optimizer.step()\n","        print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss.item():.4f}\")\n","\n","    # Evaluate on the development set\n","    model.eval()\n","    with torch.no_grad():\n","        dev_loss = 0\n","        for batch in dev_dataloader:\n","            X, lengths, Y = batch\n","            predictions = model(X, lengths)\n","            loss = criterion(predictions, Y)\n","            dev_loss += loss.item()\n","        avg_dev_loss = dev_loss/len(dev_dataloader)\n","        print(f\"Epoch {epoch + 1}/{EPOCHS}, Dev Loss: {avg_dev_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78-gPpKy3wqc","executionInfo":{"status":"ok","timestamp":1740114850512,"user_tz":-420,"elapsed":64,"user":{"displayName":"Attapol Thamrongrattanarit-Rutherford","userId":"02271080427623739850"}},"outputId":"aaa7ee56-460f-4d46-ef8e-6934badbac86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5, Loss: 1.0979\n","Epoch 1/5, Loss: 1.1265\n","Epoch 1/5, Loss: 1.0399\n","Epoch 1/5, Loss: 1.1092\n","Epoch 1/5, Loss: 1.0390\n","Epoch 1/5, Loss: 1.1533\n","Epoch 1/5, Loss: 1.0562\n","Epoch 1/5, Loss: 1.0735\n","Epoch 1/5, Dev Loss: 1.0715\n","Epoch 2/5, Loss: 0.8230\n","Epoch 2/5, Loss: 0.8895\n","Epoch 2/5, Loss: 1.1082\n","Epoch 2/5, Loss: 0.9630\n","Epoch 2/5, Loss: 0.9414\n","Epoch 2/5, Loss: 0.8723\n","Epoch 2/5, Loss: 0.7567\n","Epoch 2/5, Loss: 1.0580\n","Epoch 2/5, Dev Loss: 1.0097\n","Epoch 3/5, Loss: 0.6892\n","Epoch 3/5, Loss: 0.8620\n","Epoch 3/5, Loss: 0.7167\n","Epoch 3/5, Loss: 0.8302\n","Epoch 3/5, Loss: 0.7750\n","Epoch 3/5, Loss: 0.7402\n","Epoch 3/5, Loss: 0.8170\n","Epoch 3/5, Loss: 0.5609\n","Epoch 3/5, Dev Loss: 0.9762\n","Epoch 4/5, Loss: 0.5934\n","Epoch 4/5, Loss: 0.5630\n","Epoch 4/5, Loss: 0.5811\n","Epoch 4/5, Loss: 0.5606\n","Epoch 4/5, Loss: 0.6984\n","Epoch 4/5, Loss: 0.7167\n","Epoch 4/5, Loss: 0.5915\n","Epoch 4/5, Loss: 0.7228\n","Epoch 4/5, Dev Loss: 0.9425\n","Epoch 5/5, Loss: 0.5543\n","Epoch 5/5, Loss: 0.5800\n","Epoch 5/5, Loss: 0.5741\n","Epoch 5/5, Loss: 0.5534\n","Epoch 5/5, Loss: 0.6225\n","Epoch 5/5, Loss: 0.5523\n","Epoch 5/5, Loss: 0.5541\n","Epoch 5/5, Loss: 0.5584\n","Epoch 5/5, Dev Loss: 0.9457\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BbHUHiNd5_4n"},"execution_count":null,"outputs":[]}]}